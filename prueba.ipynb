{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                                       TP MATE 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrantes: Lucia Sannuto, Valentin Fuccenecco y Valentin Pugliese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import random\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Abro la base de datos con mis paginas almacenadas y la cargo en memoria\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "     guardados = ((pickle.load(f)))\n",
    "\n",
    "# Paginas almacenadas\n",
    "print(len(guardados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clases\n",
    "class Nodo():\n",
    "    \n",
    "    # quiero que cuando lo imprima me salga el nombre del objeto (limito los caracteres oara el grafico)\n",
    "    def __repr__(self) -> str:\n",
    "        return '%.9s' % self.nombre\n",
    "    \n",
    "    #cuando se incializa un nodo se crea en el grafico, se le da un nombre, la url de su pagina de wikipedia y se le crea una lista con todos los links dentro de ella\n",
    "    def __init__(self, nombre) -> None:\n",
    "        self.nombre = nombre\n",
    "        # ac치 se van a almacenar las urls internas\n",
    "        self.palabras = set()\n",
    "        self.parsePage(nombre)\n",
    "    \n",
    "    #metodo que va a fijarse si dos nodos tienen conexion, osea si uno referencia al otro (opcional a usar)\n",
    "    def estaEn(self, nodo):\n",
    "        if self.nombre.lower() in nodo.palabras and nodo.nombre != self.nombre:\n",
    "            return ((nodo,self))\n",
    "            \n",
    "     \n",
    "    #parsea con bs4 la pagina de wikipedia que le pases, obteniendo los hipervinculos dentro\n",
    "    def parsePage(self,nombre):\n",
    "                \n",
    "        #el prefijo para siguientes busquedas\n",
    "        prefijo_wiki = \"https://es.wikipedia.org/wiki/\"\n",
    "        url = prefijo_wiki + nombre\n",
    "\n",
    "        # Me fijo si esta ne mi base de datos de urls\n",
    "        if self.nombre in guardados.keys():\n",
    "            page = guardados[self.nombre]\n",
    "        else:\n",
    "            page = rq.get(url)\n",
    "            guardados[self.nombre] = page\n",
    "\n",
    "        #la parseo en un objeto\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        \n",
    "        #armo una lista con los terminos que yo quiero, en este caso todos los links que refertencian a otra pagina dentro de la wikipedia\n",
    "        links = soup.find_all('a', attrs={\"href\": re.compile(\"^/wiki/\")})\n",
    "\n",
    "        #selecciono a partir de que links me va a interesar revisar (es arbitrario para reducir los link que no me importan, como los de discusion e idiomas del articulo)\n",
    "        limite_inferior = 30\n",
    "\n",
    "        ## LIMPIEZA DE DATOS ##\n",
    "        \n",
    "        #obtengo el texto y lo cargo en el set\n",
    "        for i in links[limite_inferior:]:\n",
    "            self.palabras.add(i.get_text().lower().replace(\" \",\"_\")) # Hay que hacer que no sea caps sensitive y eliminar los espacios para poder usarlos en urls\n",
    "        \n",
    "        #remuevo los resultados vacios, el try hace que el remove no me explote el programa\n",
    "        try:\n",
    "            self.palabras.remove('')\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        # remuevo estos links que no me interesan\n",
    "        try:\n",
    "            self.palabras.remove(\"limitaci칩n_de_responsabilidad\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        # el about usa tampoco me interesa\n",
    "        try:\n",
    "            self.palabras.remove(\"acerca_de_wikipedia\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        # filtro aquellos resultados que empiezan con wikipedia y los del isbn, uso regex\n",
    "        for i in list(self.palabras):\n",
    "            if re.match(r\"^wikipedia\", i) or re.match(r\"^isbn\", i):\n",
    "                self.palabras.remove(i)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Ejemplo con encadenamiento ###############\n",
    "\n",
    "def logica_nodos(cant_coincidencia,nodo,nodo_anterior,vertices):\n",
    " # voy a ejecutar n coincidencias\n",
    " for i in range(cant_coincidencia):\n",
    "    # lo agrego a la lista d enodos\n",
    "    nodos.append(nodo)\n",
    "    # lo imprimo para saber nada mas\n",
    "    print(nodo.nombre)\n",
    "    # agrego el link entre el ndodo buscado y el nodo de donde sali칩\n",
    "    vertices.append((nodo_anterior, nodo)) #REVISAR ESTO, pq no funciona sin?\n",
    "    # si no esta vacio me va a servir para encontrar nuevos nodos\n",
    "    if (len(nodo.palabras)>0):\n",
    "        # checkeo con que otros nodos hay relacion (osea que esta en su lista de palabras)\n",
    "        for i in nodos:\n",
    "            if nodo.nombre in i.palabras and i.nombre != nodo.nombre: # no me interesa que se vincule consigo mismo\n",
    "                vertices.append((nodo,i))\n",
    "        # lo voy a usar para buscar mas nodos, por lo tanto lo tengo que guardar para luego vincularlo\n",
    "        nodo_anterior = nodo\n",
    "        # consigo un nuevo nodo\n",
    "        palabras_nodo_anterior = list(nodo_anterior.palabras)\n",
    "        nodo = Nodo(palabras_nodo_anterior[random.randrange(0, len(palabras_nodo_anterior))])\n",
    "        \n",
    "    else:\n",
    "        # si elnodo esta vacio uso el anterior y no me intrrumpe la ejecucion del programa\n",
    "        nodo =  Nodo(random.choice(list(nodo_anterior.palabras)))\n",
    "\n",
    " return nodo\n",
    "\n",
    "\n",
    "\n",
    "nodos = list()                          # lista que va a tener todos los nodos del grafico\n",
    "vertices = list()                       # lista que contiene los vertices\n",
    "grafo = nx.DiGraph()                    # creo el grafo\n",
    "\n",
    "nodo_central = input(\"Ingrese una palabra para buscar sus coincidencias en Wikipedia! \")\n",
    "cant_coincidencia = int (input(\"Ingrese cantidad de coincidencias \"))\n",
    "\n",
    "nodo_maestro = Nodo(nodo_central)\n",
    "primer_nodo = nodo_maestro\n",
    "\n",
    "nodos.append(nodo_maestro)              # agrego el nodo maestro a la lista\n",
    "\n",
    "nodo = Nodo(list(nodo_maestro.palabras)[random.randrange(0,len(nodo_maestro.palabras))])   # obtengo un nodo aleatorio\n",
    "nodo_anterior = nodo_maestro    # nodo que me va a servir para saber que nodo referencia a cual\n",
    "\n",
    "\n",
    "# colores de los nodos #\n",
    "color_nodos_otros = \"sandybrown\"\n",
    "color_nodo_central = \"red\"\n",
    "\n",
    "nodo = logica_nodos(cant_coincidencia,nodo,nodo_anterior,vertices) # llamo funcion \n",
    "   \n",
    "nodos = set(nodos)                # uso un set para eliminar repeticiones\n",
    "grafo.add_nodes_from(list(nodos))\n",
    "grafo.add_edges_from(vertices)\n",
    "\n",
    "node_colors = [color_nodo_central if nodos == primer_nodo else color_nodos_otros for nodos in grafo.nodes] #color central\n",
    "pos = nx.spring_layout(grafo)    #disposicion de nodos\n",
    "labels= nx.get_edge_attributes(grafo,'label')\n",
    "\n",
    "fig, ax= plt.subplots(figsize=(15,15))  # tama침o de \"universo\", para el desarrollo del grafo\n",
    "nx.draw_networkx(grafo, pos, with_labels=True, node_size=800, font_size=8,node_color=node_colors, connectionstyle=\"arc3,rad=0.2\",  arrows=True, arrowstyle='-|>', arrowsize=7)\n",
    "plt.show()\n",
    "# # para actualizar la lista de paginas (DEJAR COMENTADO)\n",
    "# with open(\"data.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(guardados,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### EJEMPLO ramificacion CENTRAL ##########\n",
    "\n",
    "nombre = input(\"Mande una palabra:\")\n",
    "\n",
    "# #lista que va a tener todos los nodos del grafico\n",
    "nodos = list()\n",
    "# #lista que contiene los vertices\n",
    "vertices = list()\n",
    "\n",
    "# #nodo central\n",
    "centro = Nodo(nombre)\n",
    "# # lo agrego a la lista con todos los nodos\n",
    "nodos.append(centro)\n",
    "\n",
    "# # obtengo un nodo aleatorio\n",
    "nodo = Nodo(list(centro.palabras)[random.randrange(0,len(centro.palabras))])\n",
    "cantidad_nodo = centro\n",
    "\n",
    "for i in range(30):\n",
    "    nodo = Nodo(list(centro.palabras)[random.randrange(0,len(centro.palabras))])\n",
    "    print(nodo.nombre)\n",
    "    if (len(nodo.palabras)>0):\n",
    "        vertices.append((centro, nodo))\n",
    "        nodos.append(nodo)\n",
    "    else:\n",
    "        nodo =  Nodo(random.choice(list(centro.palabras)))\n",
    "\n",
    "for i in nodos:\n",
    "     for j in nodos:\n",
    "        t = i.estaEn(j)\n",
    "        if t != None:\n",
    "            vertices.append(t)\n",
    "        \n",
    "# layout\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "grafo = nx.DiGraph()\n",
    "\n",
    "grafo.add_nodes_from(nodos)\n",
    "grafo.add_edges_from(vertices)\n",
    " \n",
    "pos = nx.circular_layout(grafo)\n",
    "\n",
    "#Funcion que devuelve los nodos mas importantes\n",
    "def mas_relevancia(grafo):\n",
    "#copia de grafo con los nodos mas importantes \n",
    "    ranking = nx.betweenness_centrality(grafo).items()\n",
    "    r = [x[1] for x in ranking]\n",
    "\n",
    "    m = sum(r)/len(r) # centralidad\n",
    "\n",
    "    t = m*3 # mantiene los nodos con promedio de aparicion --> si queres que sean mas o menos los coloreados en rojo modifica esto\n",
    "   \n",
    "    nodo_importante = grafo.copy()\n",
    "\n",
    "    #Se remueve los nodos que no aparecen\n",
    "    for k, v in ranking:\n",
    "        if v < t:\n",
    "            nodo_importante.remove_node(k)\n",
    "   \n",
    "    return nodo_importante\n",
    "\n",
    "\n",
    "#Se recibe los nodos mas importantes\n",
    "nodo_importante = mas_relevancia(grafo)\n",
    "\n",
    "\n",
    "nx.draw(grafo,\n",
    "        pos,\n",
    "        node_color='orange',\n",
    "        alpha=0.2,\n",
    "        node_size=800, \n",
    "        with_labels = True, \n",
    "        connectionstyle = \"arc3, rad=0.5\",\n",
    "        font_size = 12\n",
    "       )\n",
    "\n",
    "\n",
    "#Se dibuja los nodos mas importantes con un estilo diferente\n",
    "for i in range(0,len(nodo_importante)):\n",
    "    grafo.add_nodes_from(nodo_importante)\n",
    "    nx.draw_networkx_nodes(nodo_importante,\n",
    "                           pos,\n",
    "                           node_color='r',\n",
    "                           alpha=0.4,\n",
    "                           node_size=1000)\n",
    "    nx.draw_networkx_labels(nodo_importante,pos,font_size=12)\n",
    "\n",
    "\n",
    "#Se muestra la grafica\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
